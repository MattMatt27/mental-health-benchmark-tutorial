{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38c4c639",
   "metadata": {},
   "source": "# Build Your Own Benchmark\n\nIn Notebook 01, we explored the pre-computed results of administering the SIRI-2 to nine language models. This notebook walks through **running your own evaluation** from scratch: making live API calls, scoring the results, and comparing your model to published human benchmarks.\n\n### Why run your own evaluation?\n\nThe central finding of our study is that model performance varies dramatically depending on how the model is prompted and configured. A model that scores at the level of a trained counselor under one set of conditions can score at the level of an untrained undergraduate under another. The only way to know how a model performs in *your* context — with your prompts, your clinical domain, your scoring criteria — is to test it yourself.\n\nThis notebook makes that process accessible. Every API call is a simple request-response: you send the model a clinical scenario and a helper response, and it sends back a rating. No machine learning expertise is required. If you can interpret a SIRI-2 score, you can interpret the output of this pipeline.\n\n### What you'll need\n\n- **At least one API key** from OpenAI, Anthropic, or Google. The setup cell below has a place to paste your key directly. If you don't have an API key yet, each provider offers a free or low-cost tier — see the README for links.\n- **Python dependencies** installed (`pip install -r requirements.txt`).\n\n### What this costs\n\nThe tutorial run below uses a single model, one prompt variant, one temperature, and three repetitions — about **144 API calls**. This costs a few cents and completes in under a minute. Section 8 shows how to scale up to the full experiment design (~$100 for all nine models)."
  },
  {
   "cell_type": "markdown",
   "id": "073a1e71",
   "metadata": {},
   "source": "---\n## 1. Setup\n\nThe cell below loads libraries and detects your API key. **You have two options** for providing your key:\n\n1. **Paste it directly** in the cell below (simplest — uncomment one line and paste your key)\n2. **Use a `.env` file** if you prefer to keep keys out of the notebook. Note: `.env` files are hidden by default in Jupyter Lab. To see them, go to **View > Show Hidden Files** in the Jupyter Lab menu bar, then copy `.env.example` to `.env` and fill in your key(s).\n\nEither way works. If you paste a key directly, it stays in your local notebook and is never sent anywhere except to the API provider."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b64c580",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport json\nimport sys\nimport os\nfrom pathlib import Path\n\n# ── Paths ─────────────────────────────────────────────────────────────\nREPO_ROOT = Path('..').resolve()\nINSTRUMENT_DIR = REPO_ROOT / 'instrument'\n\n# Tutorial results go in their own directory so they don't mix with\n# the published experiment data in experiment-results/\nTUTORIAL_OUTPUT = REPO_ROOT / 'tutorial-results'\n\n# ── API keys ──────────────────────────────────────────────────────────\n# OPTION 1: Paste your key directly (uncomment ONE line below).\n# os.environ['ANTHROPIC_API_KEY'] = 'sk-ant-...'   # Anthropic\n# os.environ['OPENAI_API_KEY']    = 'sk-...'        # OpenAI\n# os.environ['GOOGLE_API_KEY']    = 'AI...'          # Google\n\n# OPTION 2: Load from .env file (works automatically if .env exists).\nfrom dotenv import load_dotenv\nload_dotenv(REPO_ROOT / '.env')\n\n# Make the src/ modules importable from this notebook\nsys.path.insert(0, str(REPO_ROOT / 'src'))\n\n# Plot styling\nplt.style.use('seaborn-v0_8-whitegrid')\npd.set_option('display.max_columns', 15)\npd.set_option('display.width', 120)\n\n# ── Detect which providers are available ──────────────────────────────\n# Each provider needs its own API key. The cheapest/fastest model for\n# each provider is listed as the default tutorial model.\nAVAILABLE_MODELS = {}\nif os.getenv('ANTHROPIC_API_KEY'):\n    AVAILABLE_MODELS['anthropic'] = 'claude-3-5-haiku-20241022'\nif os.getenv('GOOGLE_API_KEY'):\n    AVAILABLE_MODELS['google'] = 'gemini-2.0-flash'\nif os.getenv('OPENAI_API_KEY'):\n    AVAILABLE_MODELS['openai'] = 'gpt-3.5-turbo-0125'\n\nif not AVAILABLE_MODELS:\n    raise RuntimeError(\n        'No API keys found.\\n\\n'\n        'Uncomment one of the os.environ lines at the top of this cell\\n'\n        'and paste your API key, then re-run this cell.'\n    )\n\n# Pick the first available model (provider order prioritizes cost)\nMODEL = list(AVAILABLE_MODELS.values())[0]\n\nprint(f'Available providers: {list(AVAILABLE_MODELS.keys())}')\nprint(f'Auto-selected model: {MODEL}')\nprint(f'\\nTo use a different model, change MODEL in the configuration cell below.')\nprint(f'Tutorial output will be saved to: {TUTORIAL_OUTPUT}')"
  },
  {
   "cell_type": "markdown",
   "id": "612d23a7",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Review the Instrument\n",
    "\n",
    "Before running the experiment, let's look at what the models will be asked to evaluate. The SIRI-2 instrument is stored as two files:\n",
    "\n",
    "1. **`siri2_items.json`** — the 24 clinical scenarios, each presenting a client expressing suicidal ideation and two possible helper responses\n",
    "2. **`siri2_expert_scores.csv`** — the expert panel's mean ratings and standard deviations for all 48 helper responses (24 scenarios x 2 helpers)\n",
    "\n",
    "The model receives exactly what a human respondent would: a client statement, a single helper response, and instructions to rate it on the -3 to +3 scale. Each helper response is scored independently — the model never sees both options for the same scenario at once, just as a human respondent rates each one separately.\n",
    "\n",
    "Any instrument that follows this same format will work with the benchmark runner (see Section 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9c9d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the instrument items\n",
    "with open(INSTRUMENT_DIR / 'siri2_items.json') as f:\n",
    "    items = json.load(f)\n",
    "\n",
    "print(f'{len(items)} scenarios\\n')\n",
    "\n",
    "# Show one scenario as an example.\n",
    "# Each scenario has a client statement and two helper responses.\n",
    "# In clinical terms: this is a client in crisis, and we're asking\n",
    "# the model to evaluate how appropriate each possible helper response is.\n",
    "item = items[0]\n",
    "print(f'Scenario {item[\"item_id\"]}:')\n",
    "print(f'  Client: \"{item[\"client\"]}\"')\n",
    "print(f'  Helper A: \"{item[\"helper_a\"]}\"')\n",
    "print(f'  Helper B: \"{item[\"helper_b\"]}\"')\n",
    "print(f'\\nRequired JSON fields: item_id, client, helper_a, helper_b')\n",
    "print(f'The model will rate Helper A and Helper B separately.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7dfdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the expert scoring key.\n",
    "# These are the ground-truth ratings from a panel of seven nationally\n",
    "# recognized suicidologists. \"M\" is the panel mean, \"SD\" is the standard\n",
    "# deviation. Small SDs mean the experts largely agreed on the rating.\n",
    "expert = pd.read_csv(INSTRUMENT_DIR / 'siri2_expert_scores.csv')\n",
    "expert.columns = ['Item', 'expert_mean', 'expert_sd']\n",
    "\n",
    "print(f'{len(expert)} scored items (24 scenarios x 2 helpers = 48 items)')\n",
    "print(f'Rating scale: -3 (very inappropriate) to +3 (very appropriate)\\n')\n",
    "print(expert.head(10).to_string(index=False))\n",
    "print(f'\\nNote: Item 14 is included in the data but excluded from scoring.')\n",
    "print(f'The original expert panel could not reach consensus on it — both')\n",
    "print(f'helper responses represent defensible crisis intervention strategies.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0893e4b8",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Configure the Experiment\n",
    "\n",
    "The benchmark runner supports four configuration axes:\n",
    "\n",
    "| Parameter | What it controls | Full experiment | Tutorial run |\n",
    "|-----------|-----------------|----------------|---------------|\n",
    "| **Models** | Which LLMs to test | 9 (across 3 providers) | 1 |\n",
    "| **Prompt variants** | How much clinical context the model receives | 3 | 1 (detailed) |\n",
    "| **Temperatures** | How much randomness in the model's output | 2 (0.0, 1.0) | 1 (1.0) |\n",
    "| **Repetitions** | How many times to ask the same question | 10 | 3 |\n",
    "\n",
    "A few notes on what these mean in practice:\n",
    "\n",
    "- **Temperature** controls randomness. At temperature 0, the model gives the same answer every time (deterministic). At temperature 1.0, there is stochastic variation — the model may rate the same item differently across repetitions. We use 1.0 here so you can see how consistent the model is. A model that gives wildly different ratings to the same scenario on repeated trials is less trustworthy, even if its average is close to the expert mean.\n",
    "\n",
    "- **Prompt variant** determines how much clinical context the model receives before rating each item. The \"detailed\" prompt frames the task as rating an excerpt from a counseling session on the SIRI-2 scale. Notebook 01 (Section 4) showed that this framing substantially improves performance for most models — without it, models often default to generic sentiment rather than clinical judgment.\n",
    "\n",
    "- **Repetitions** let us measure both the average response and its variability. Three repetitions is enough to get a rough estimate; the full study used ten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fd81fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Tutorial configuration ──────────────────────────────────────────\n",
    "# MODEL was auto-selected above based on your API keys.\n",
    "# To use a different model, uncomment one of the lines below.\n",
    "#\n",
    "# Available models (must match your API key):\n",
    "#   Anthropic: claude-3-5-haiku-20241022, claude-3-5-sonnet-20241022,\n",
    "#             claude-sonnet-4-20250514, claude-opus-4-20250514\n",
    "#   OpenAI:   gpt-3.5-turbo-0125, gpt-4o-2024-11-20\n",
    "#   Google:   gemini-2.0-flash, gemini-2.5-flash, gemini-2.5-pro\n",
    "\n",
    "# MODEL = 'claude-3-5-haiku-20241022'\n",
    "\n",
    "TEMPERATURE = 1.0       # stochastic — lets us measure consistency\n",
    "PROMPT_VARIANT = 'detailed'  # provides full clinical context\n",
    "REPEATS = 3             # enough to estimate mean and variability\n",
    "\n",
    "# Show what the experiment will look like\n",
    "n_calls = 1 * 1 * 1 * len(items) * 2 * REPEATS\n",
    "print(f'Model:           {MODEL}')\n",
    "print(f'Temperature:     {TEMPERATURE}')\n",
    "print(f'Prompt variant:  {PROMPT_VARIANT}')\n",
    "print(f'Repetitions:     {REPEATS}')\n",
    "print(f'Scenarios:       {len(items)} (x2 helpers each = {len(items) * 2} items)')\n",
    "print(f'\\nTotal API calls: {len(items)} scenarios x 2 helpers x {REPEATS} repeats = {n_calls}')\n",
    "print(f'Estimated time:  ~1 minute')\n",
    "print(f'Estimated cost:  a few cents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc0656e",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Run the Experiment\n",
    "\n",
    "This is the step that actually talks to the model. The `run_experiment()` function sends each helper response to the model as an **independent API call** with no shared conversation context. This means the model's rating of Item 1A cannot influence its rating of Item 1B or any other item — there are no order effects or anchoring between items, just as in a properly administered paper-and-pencil assessment.\n",
    "\n",
    "Results are written incrementally to a JSONL file (one JSON record per line). **If the run is interrupted** (network issue, kernel restart, etc.), re-running the cell picks up where it left off — completed calls are detected and skipped automatically. You will not be double-charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2101d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_runner import run_experiment, summarize\n",
    "\n",
    "# This cell makes live API calls. It will take about a minute.\n",
    "# A progress bar will appear showing each item being scored.\n",
    "run_experiment(\n",
    "    prompts_path=INSTRUMENT_DIR / 'siri2_items.json',\n",
    "    output_dir=TUTORIAL_OUTPUT,\n",
    "    models=[MODEL],\n",
    "    temperatures=[TEMPERATURE],\n",
    "    prompt_variants=[PROMPT_VARIANT],\n",
    "    repeats=REPEATS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da577985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at what the model actually returned for one API call.\n",
    "# Each record captures the model's numeric rating (\"score\") plus\n",
    "# metadata about which item, helper, and repetition it was.\n",
    "with open(TUTORIAL_OUTPUT / 'api_responses_raw.jsonl') as f:\n",
    "    first = json.loads(f.readline())\n",
    "\n",
    "print('Fields recorded for each API call:\\n')\n",
    "for key in ['model', 'temperature', 'prompt_variant', 'item_id', 'helper',\n",
    "            'repeat', 'score', 'reasoning', 'raw_response']:\n",
    "    val = first.get(key)\n",
    "    if isinstance(val, str) and len(val) > 80:\n",
    "        val = val[:80] + '...'\n",
    "    print(f'  {key}: {val}')\n",
    "\n",
    "print(f'\\nThe \"score\" field is the model\\'s rating on the -3 to +3 scale.')\n",
    "print(f'The \"reasoning\" field is null here because the \"detailed\" prompt')\n",
    "print(f'only asks for a score. The \"detailed_w_reasoning\" variant also')\n",
    "print(f'asks the model to explain its rating in free text.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6886ce",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Summarize Results\n",
    "\n",
    "The raw JSONL file contains one record per API call — with 3 repetitions per item, that means 3 separate ratings for each of the 48 helper responses. The `summarize()` function aggregates these into a single row per item with the **mean** rating (average across repetitions), **standard deviation** (how much the ratings varied), and **count** (number of repetitions).\n",
    "\n",
    "This is analogous to computing a respondent's average score when they take a test multiple times. The mean tells you where the model lands; the SD tells you how stable that landing is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94337b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate raw responses: compute mean, std, and count across repetitions\n",
    "summarize(TUTORIAL_OUTPUT)\n",
    "\n",
    "# Load the summary table\n",
    "summary = pd.read_csv(TUTORIAL_OUTPUT / 'model_scores_by_condition.csv')\n",
    "print(f'\\n{len(summary)} rows (one per item x helper)')\n",
    "print(f'Columns: {summary.columns.tolist()}')\n",
    "print(f'\\nKey columns:')\n",
    "print(f'  mean  = average score across {REPEATS} repetitions')\n",
    "print(f'  std   = standard deviation across repetitions (consistency)')\n",
    "print(f'  count = number of repetitions\\n')\n",
    "summary.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c351928",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Score Against Experts\n",
    "\n",
    "Now we compute the SIRI-2 total score — the same metric used in every published human study of this instrument. For each of the 48 validated items, we take the absolute difference between the model's mean rating and the expert panel's mean rating, then sum across all items.\n",
    "\n",
    "$$\\text{SIRI-2 Total Score} = \\sum_{i=1}^{48} |\\text{model mean}_i - \\text{expert mean}_i|$$\n",
    "\n",
    "The result is a single number: **lower is better**. A score of 0 would mean perfect agreement with the expert panel on every item. The expected score for an individual expert panelist (rating against the panel mean) is approximately 32.5 — this represents the natural disagreement *within* the expert panel itself. Scores below this threshold indicate performance that exceeds the typical agreement among the experts themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db50abcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Step 1: Merge model scores with expert ratings ────────────────────\n",
    "# This joins each item's model mean with the corresponding expert mean,\n",
    "# so we can compute the distance between them.\n",
    "merged = summary.merge(expert, on='Item', how='inner')\n",
    "\n",
    "# ── Step 2: Exclude item 14 (not validated) ───────────────────────────\n",
    "merged = merged[merged['item_id'] != 14].copy()\n",
    "\n",
    "# ── Step 3: Compute absolute error per item ───────────────────────────\n",
    "# For each item: how far was the model's average rating from the expert mean?\n",
    "# Example: if experts rated an item -2.71 and the model averaged -1.5,\n",
    "# the absolute error is |(-1.5) - (-2.71)| = 1.21\n",
    "merged['abs_error'] = (merged['mean'] - merged['expert_mean']).abs()\n",
    "\n",
    "# ── Step 4: Sum to get the SIRI-2 total score ────────────────────────\n",
    "siri2_score = merged['abs_error'].sum()\n",
    "n_items = len(merged)\n",
    "mae = siri2_score / n_items\n",
    "\n",
    "print(f'SIRI-2 Total Score: {siri2_score:.1f}')\n",
    "print(f'  Items scored: {n_items}')\n",
    "print(f'  Mean absolute error per item: {mae:.2f}')\n",
    "print(f'  Model: {MODEL}')\n",
    "print(f'  Condition: {PROMPT_VARIANT}, T={TEMPERATURE}, {REPEATS} repeats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc664208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Compare to published human benchmarks ─────────────────────────────\n",
    "# The SIRI-2 has been administered to a wide range of clinical and\n",
    "# non-clinical groups over the past 30 years. These published scores\n",
    "# let us place the model on the same scale as real clinicians.\n",
    "from siri2_scoring import HUMAN_BENCHMARKS\n",
    "\n",
    "EXPERT_BASELINE = 32.5  # expected score for an individual expert panelist\n",
    "\n",
    "# Key reference points from the SIRI-2 literature\n",
    "refs = [\n",
    "    ('Expert panel baseline', EXPERT_BASELINE),\n",
    "    (\"Master's counselors (post-training)\", 41.0),\n",
    "    ('Clinical psych PhD students', 45.4),\n",
    "    ('K-12 school staff (pre-training)', 52.9),\n",
    "    (\"Master's counselors (pre-training)\", 54.7),\n",
    "    ('Intro psychology students', 70.4),\n",
    "]\n",
    "\n",
    "print(f'Your model scored {siri2_score:.1f}. Here\\'s how that compares:\\n')\n",
    "print(f'{\"Respondent\":<42s}  {\"SIRI-2 Score\":>12s}')\n",
    "print(f'{\"-\" * 42}  {\"-\" * 12}')\n",
    "\n",
    "for label, score in sorted(refs + [(f'>>> {MODEL}', siri2_score)], key=lambda x: x[1]):\n",
    "    display = label.replace('>>> ', '')\n",
    "    if label.startswith('>>>'):\n",
    "        print(f'  {display:<42s}  {score:>10.1f}  <-- your run')\n",
    "    else:\n",
    "        print(f'  {display:<42s}  {score:>10.1f}')\n",
    "\n",
    "print(f'\\nScores below 32.5 mean the model agrees with experts more closely')\n",
    "print(f'than a typical individual expert agrees with the rest of the panel.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2683c8e2",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Visualize Your Results\n",
    "\n",
    "### Per-item comparison\n",
    "\n",
    "The chart below shows the expert panel mean (green) alongside your model's mean rating (blue) for each of the 48 validated items. Error bars on the blue bars show the standard deviation across repetitions — wider bars mean the model was less consistent on that item.\n",
    "\n",
    "**What to look for:**\n",
    "- **Close alignment:** green and blue bars of similar height mean the model agrees with experts.\n",
    "- **Sign reversals:** if a green bar points up (appropriate) but the blue bar points down (inappropriate), or vice versa, the model and experts fundamentally disagree about that response. This is the most clinically concerning type of error.\n",
    "- **Consistent overrating:** if blue bars are systematically taller than green bars on inappropriate items (left side of zero), the model is rating bad responses too favorably — a pattern we found across most models in the full study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e14f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort items in order (1A, 1B, 2A, 2B, ...) for a clean plot\n",
    "plot_data = merged.sort_values('Item').copy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "x = np.arange(len(plot_data))\n",
    "width = 0.35\n",
    "\n",
    "# Green bars = expert panel means (ground truth)\n",
    "ax.bar(x - width/2, plot_data['expert_mean'], width, color='#2ca02c', alpha=0.7,\n",
    "       label='Expert panel mean')\n",
    "\n",
    "# Blue bars = model's mean ratings\n",
    "ax.bar(x + width/2, plot_data['mean'], width, color='#4a90d9', alpha=0.7,\n",
    "       label=f'{MODEL}')\n",
    "\n",
    "# Error bars showing model variability across repetitions\n",
    "if 'std' in plot_data.columns and plot_data['std'].notna().any():\n",
    "    ax.errorbar(x + width/2, plot_data['mean'], yerr=plot_data['std'],\n",
    "                fmt='none', ecolor='#4a90d9', alpha=0.4, capsize=1.5)\n",
    "\n",
    "ax.axhline(0, color='black', linewidth=0.5)\n",
    "ax.set_xlabel('Item', fontsize=10)\n",
    "ax.set_ylabel('Rating (-3 to +3)', fontsize=10)\n",
    "ax.set_title(f'Expert vs. Model Ratings by Item ({MODEL})', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(plot_data['Item'], fontsize=6, rotation=90)\n",
    "ax.legend(fontsize=9)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Flag sign reversals — items where model and expert disagree on the\n",
    "# basic direction (appropriate vs. inappropriate). These are the most\n",
    "# clinically significant errors.\n",
    "reversals = plot_data[\n",
    "    (plot_data['mean'] * plot_data['expert_mean'] < 0) &\n",
    "    (plot_data['mean'].abs() > 0.5)\n",
    "]\n",
    "if len(reversals) > 0:\n",
    "    print(f'Sign reversals (model and expert on opposite sides of zero):')\n",
    "    for _, r in reversals.iterrows():\n",
    "        direction = 'appropriate' if r['expert_mean'] > 0 else 'inappropriate'\n",
    "        print(f'  Item {r[\"Item\"]}: expert = {r[\"expert_mean\"]:+.2f} ({direction}), '\n",
    "              f'model = {r[\"mean\"]:+.2f}')\n",
    "else:\n",
    "    print('No sign reversals detected — the model agrees with experts on the')\n",
    "    print('basic direction (appropriate vs. inappropriate) for all items.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cc7c1a",
   "metadata": {},
   "source": [
    "### Score in context\n",
    "\n",
    "The chart below places your model's SIRI-2 total score (red star) among published human benchmark groups (gray squares) and the expert panel baseline (green diamond). These human groups span a wide range of clinical training and experience — from untrained introductory psychology students to post-training master's-level counselors.\n",
    "\n",
    "This is the same comparison that has been used in the SIRI-2 literature for 30 years to contextualize the clinical competence of new respondent groups. The only difference is that the respondent here is a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da57878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the comparison dataset: human benchmark groups + your model\n",
    "entries = [{'label': h['group'], 'score': h['score'], 'type': 'human'}\n",
    "           for h in HUMAN_BENCHMARKS if 'chat tool' not in h['group']]\n",
    "entries.append({'label': 'Expert panel baseline', 'score': EXPERT_BASELINE, 'type': 'expert'})\n",
    "entries.append({'label': f'{MODEL} (your run)', 'score': siri2_score, 'type': 'model'})\n",
    "entries = sorted(entries, key=lambda x: x['score'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, max(6, len(entries) * 0.28)))\n",
    "\n",
    "# Different markers for each type of entry\n",
    "colors = {'human': '#5a6872', 'expert': '#2ca02c', 'model': '#d64541'}\n",
    "markers = {'human': 's', 'expert': 'D', 'model': '*'}\n",
    "sizes = {'human': 5, 'expert': 7, 'model': 12}\n",
    "\n",
    "for i, entry in enumerate(entries):\n",
    "    t = entry['type']\n",
    "    ax.plot(entry['score'], i, markers[t], color=colors[t],\n",
    "            markersize=sizes[t], markeredgecolor='white', markeredgewidth=0.5, zorder=3)\n",
    "\n",
    "# Style the y-axis labels to match the marker colors\n",
    "ax.set_yticks(range(len(entries)))\n",
    "ax.set_yticklabels([e['label'] for e in entries], fontsize=8)\n",
    "for i, entry in enumerate(entries):\n",
    "    tick = ax.get_yticklabels()[i]\n",
    "    tick.set_color(colors[entry['type']])\n",
    "    if entry['type'] == 'model':\n",
    "        tick.set_fontweight('bold')\n",
    "\n",
    "# Dashed line at expert baseline for reference\n",
    "ax.axvline(EXPERT_BASELINE, color='#2ca02c', linewidth=1, linestyle='--', alpha=0.5, zorder=1)\n",
    "\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('SIRI-2 Total Score (lower = better)', fontsize=10)\n",
    "ax.set_title('Your Model vs. Human Benchmarks', fontsize=12, fontweight='bold')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.grid(axis='x', alpha=0.2)\n",
    "ax.tick_params(axis='y', length=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f4c172",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Scaling Up\n",
    "\n",
    "The tutorial run above used a single model and condition. To run a more comprehensive evaluation, you can expand any of the configuration parameters. The benchmark runner handles the combinatorics automatically — it crosses every model with every temperature, prompt variant, and repetition count.\n",
    "\n",
    "The full experiment design from the paper crosses **9 models x 3 prompt variants x 2 temperatures x 10 repetitions**, producing about 27,000 API calls at a total cost of roughly $100. The bulk of that cost comes from the larger models (Claude Opus 4, GPT-4o, Gemini 2.5 Pro). A more targeted evaluation — say, 3 models, 2 prompts, both temperatures, 5 repetitions — would cost under $10.\n",
    "\n",
    "Because the runner is **resumable**, you can scale up incrementally. Run one model today, add another tomorrow. Already-completed calls are skipped automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516e8bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: full experiment configuration\n",
    "# (Don't run this cell unless you intend to — it will make ~27,000 API calls.)\n",
    "# Requires API keys for all three providers (OpenAI, Anthropic, Google).\n",
    "\n",
    "FULL_CONFIG = \"\"\"\n",
    "run_experiment(\n",
    "    prompts_path=INSTRUMENT_DIR / 'siri2_items.json',\n",
    "    output_dir=REPO_ROOT / 'my-full-results',\n",
    "    models=[\n",
    "        'gpt-3.5-turbo-0125', 'gpt-4o-2024-11-20',              # OpenAI\n",
    "        'claude-3-5-haiku-20241022', 'claude-3-5-sonnet-20241022',\n",
    "        'claude-sonnet-4-20250514', 'claude-opus-4-20250514',     # Anthropic\n",
    "        'gemini-2.0-flash', 'gemini-2.5-flash', 'gemini-2.5-pro', # Google\n",
    "    ],\n",
    "    temperatures=[0.0, 1.0],\n",
    "    prompt_variants=['minimal', 'detailed', 'detailed_w_reasoning'],\n",
    "    repeats=10,\n",
    ")\n",
    "\"\"\"\n",
    "print(FULL_CONFIG)\n",
    "print('For deeper analysis on full results, the src/ directory includes:')\n",
    "print('  analysis.py           — RMSE decomposition, prompt/temperature effect figures')\n",
    "print('  siri2_scoring.py      — SIRI-2 total scores and human benchmark comparison')\n",
    "print('  tutorial_figures.py   — publication-quality figures from the paper')\n",
    "print('  reasoning_analysis.py — embedding-based reasoning consistency (advanced)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1911e70",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Adapting for Your Own Instrument\n",
    "\n",
    "The benchmark runner works with **any** clinical instrument that follows the same format. The SIRI-2 is used here because it provides a well-validated scoring key and decades of published human comparison data, but the pipeline itself is instrument-agnostic. If you have a clinical assessment that presents scenarios with ratable responses, you can plug it in.\n",
    "\n",
    "To substitute your own instrument, you need two files:\n",
    "\n",
    "### Items file (JSON)\n",
    "\n",
    "A JSON array where each object has four fields:\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"item_id\": \"01\",\n",
    "    \"client\": \"The client statement or scenario description.\",\n",
    "    \"helper_a\": \"One possible helper/clinician response.\",\n",
    "    \"helper_b\": \"An alternative helper/clinician response.\"\n",
    "  },\n",
    "  ...\n",
    "]\n",
    "```\n",
    "\n",
    "Each helper response is sent to the model independently — the model sees only the client statement and one helper response at a time. This mirrors how the SIRI-2 is administered to human respondents.\n",
    "\n",
    "### Expert scoring key (CSV)\n",
    "\n",
    "A CSV with three columns:\n",
    "\n",
    "```\n",
    "Item,M,SD\n",
    "1A,-2.71,0.49\n",
    "1B,+1.86,0.38\n",
    "2A,-2.71,0.49\n",
    "...\n",
    "```\n",
    "\n",
    "The `Item` column uses the format `{item_number}{A|B}` to match each helper response. `M` is the expert panel mean and `SD` is the standard deviation. The scoring pipeline uses `M` to compute alignment; `SD` is used for the expert baseline calculation and for understanding items where experts themselves disagreed.\n",
    "\n",
    "### Prompt text\n",
    "\n",
    "You will also want to modify the system prompt to match your instrument's rating scale and clinical context. The prompt variants are defined in `src/benchmark_runner.py` in the `PROMPT_VARIANTS` dictionary. The key elements to customize are:\n",
    "\n",
    "1. The **rating scale** — the SIRI-2 uses -3 to +3; your instrument may use a different range (e.g., 1-5, 0-10)\n",
    "2. The **clinical framing** — \"hypothetical counseling session\" may not fit your domain. A diagnostic reasoning instrument would need different framing than a therapeutic alliance measure.\n",
    "3. The **output format** — JSON with `score` and optionally `reasoning`. The parser handles common variations (markdown code blocks, leading + signs, surrounding prose).\n",
    "\n",
    "### Assembling an expert panel\n",
    "\n",
    "The scoring key is the most important component of any benchmark — it defines ground truth. For a new instrument, this requires recruiting qualified experts to independently rate each item and computing the panel means. The SIRI-2 used seven nationally recognized suicidologists; modern psychometric practice generally recommends larger and more diverse panels. The key criterion is that panelists should be people whose judgment you would trust in the clinical domain the instrument covers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96849f04",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Next Steps\n",
    "\n",
    "This notebook demonstrated the complete pipeline from API call to scored benchmark result. You sent a clinical instrument to a language model, collected its ratings, scored them against expert consensus, and placed the result in the context of three decades of published human performance data — all in a few minutes and for a few cents.\n",
    "\n",
    "From here, you can:\n",
    "\n",
    "- **Scale up** to more models, prompt variants, and temperatures (Section 8). The runner is resumable, so you can add conditions incrementally.\n",
    "- **Substitute your own instrument** following the format described in Section 9. The scoring code works identically regardless of clinical content.\n",
    "- **Run deeper analysis** using the modules in `src/` — RMSE decomposition, per-item bias analysis, and reasoning embedding consistency.\n",
    "- **Compare to the full results** from our nine-model experiment in Notebook 01.\n",
    "\n",
    "### Get involved\n",
    "\n",
    "The research team behind this paper is building **MindBench.ai**, a benchmarking aggregation project for mental health AI. We are assembling a growing library of expert-rated clinical scenarios across crisis intervention, therapeutic alliance, diagnostic reasoning, and other domains where clinical expertise is essential for AI evaluation.\n",
    "\n",
    "There are two ways to contribute:\n",
    "\n",
    "1. **Rate existing benchmark items** — bring your clinical perspective to instruments like the SIRI-2. Your areas of agreement and disagreement with existing scoring keys are informative, particularly if they reflect clinical perspectives that original expert panels may not have represented.\n",
    "2. **Write new scenarios** — develop items for domains that existing instruments don't cover. Clinicians who write scenarios for training exercises, supervision, or competency exams are already producing material in this format.\n",
    "\n",
    "If you are interested in contributing, please contact the corresponding author."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_jupyter",
   "language": "python",
   "name": "venv_jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}